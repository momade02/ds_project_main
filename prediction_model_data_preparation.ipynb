{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ca75cb0",
   "metadata": {},
   "source": [
    "# prediction_model data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "247ca38f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IOException",
     "evalue": "IO Error: File is already open in \nC:\\Users\\websi\\anaconda3\\python.exe (PID 12888)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOException\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 22\u001b[0m\n\u001b[0;32m     16\u001b[0m STATION_GLOBS \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mstr\u001b[39m(BASE_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstations\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*-stations.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mstr\u001b[39m(BASE_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstations\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2024\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*-stations.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m ]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Connect and setup\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m con \u001b[38;5;241m=\u001b[39m duckdb\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;28mstr\u001b[39m(DB_PATH))\n\u001b[0;32m     23\u001b[0m con\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRAGMA threads=8;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m con\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT setseed(0.42);\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Crucial for reproducible sampling\u001b[39;00m\n",
      "\u001b[1;31mIOException\u001b[0m: IO Error: File is already open in \nC:\\Users\\websi\\anaconda3\\python.exe (PID 12888)"
     ]
    }
   ],
   "source": [
    "# 1. Configuration & Connection\n",
    "\n",
    "import duckdb\n",
    "from pathlib import Path\n",
    "\n",
    "# Use relative path or home-based path to make it portable\n",
    "BASE_DIR = Path(r\"C:\\Users\\websi\\OneDrive - UT Cloud\\Semester\\3. WS2025_26\\DS500 Data Science Project (12 ECTS)\\tankerkoenig_repo\\tankerkoenig-data\")\n",
    "DB_PATH = BASE_DIR / \"fuel_price_preparation.duckdb\"\n",
    "OUTPUT_PARQUET = BASE_DIR / \"derived\" / \"features_sampled_e5_2023_2024.parquet\"\n",
    "\n",
    "# Globs for 2023/2024\n",
    "PRICE_GLOBS = [\n",
    "    str(BASE_DIR / \"prices\" / \"2023\" / \"*\" / \"*-prices.csv\"),\n",
    "    str(BASE_DIR / \"prices\" / \"2024\" / \"*\" / \"*-prices.csv\")\n",
    "]\n",
    "STATION_GLOBS = [\n",
    "    str(BASE_DIR / \"stations\" / \"2023\" / \"*\" / \"*-stations.csv\"),\n",
    "    str(BASE_DIR / \"stations\" / \"2024\" / \"*\" / \"*-stations.csv\")\n",
    "]\n",
    "\n",
    "# Connect and setup\n",
    "con = duckdb.connect(str(DB_PATH))\n",
    "con.execute(\"PRAGMA threads=8;\")\n",
    "con.execute(\"SELECT setseed(0.42);\") # Crucial for reproducible sampling\n",
    "print(f\"Connected to: {DB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbe4865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Ingest Prices and Stations\n",
    "con.execute(\"DROP TABLE IF EXISTS prices_raw; DROP TABLE IF EXISTS stations_raw;\")\n",
    "\n",
    "query_ingest = \"\"\"\n",
    "    -- Read all price files at once\n",
    "    CREATE TABLE prices_raw AS \n",
    "    SELECT * FROM read_csv_auto(?, SAMPLE_SIZE=-1);\n",
    "\n",
    "    -- Read all station files at once, keeping filename for snapshot logic\n",
    "    CREATE TABLE stations_raw AS \n",
    "    SELECT * FROM read_csv_auto(?, SAMPLE_SIZE=-1, filename=true, union_by_name=true);\n",
    "\"\"\"\n",
    "\n",
    "con.execute(query_ingest, [PRICE_GLOBS, STATION_GLOBS])\n",
    "\n",
    "# Sanity Check\n",
    "print(\"Ingestion Complete.\")\n",
    "con.sql(\"SELECT 'Prices' as type, count(*) as n FROM prices_raw UNION ALL SELECT 'Stations', count(*) FROM stations_raw\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e8fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Station Processing: Snapshot -> Sample -> Brand Grouping\n",
    "con.execute(\"DROP TABLE IF EXISTS stations_final_sample;\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE stations_final_sample AS\n",
    "WITH parsed_stations AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        -- Extract date from filename (e.g., 2023-01-01)\n",
    "        CAST(regexp_extract(filename, '([0-9]{4}-[0-9]{2}-[0-9]{2})', 1) AS DATE) AS snapshot_date\n",
    "    FROM stations_raw\n",
    "),\n",
    "latest_snapshot AS (\n",
    "    SELECT * EXCLUDE (rn) FROM (\n",
    "        SELECT *, ROW_NUMBER() OVER (PARTITION BY uuid ORDER BY snapshot_date DESC) as rn\n",
    "        FROM parsed_stations\n",
    "    ) WHERE rn = 1\n",
    "),\n",
    "sampled_stations AS (\n",
    "    -- Randomly sample 500 stations from the latest snapshots\n",
    "    -- Note: seed 0.42 was set in Cell 1, so this is deterministic\n",
    "    SELECT * FROM latest_snapshot ORDER BY random() LIMIT 500\n",
    "),\n",
    "brand_counts AS (\n",
    "    SELECT brand, COUNT(*) as n FROM sampled_stations GROUP BY brand\n",
    ")\n",
    "SELECT \n",
    "    s.uuid AS station_uuid,\n",
    "    s.city,\n",
    "    s.brand,\n",
    "    CASE WHEN bc.n > 5 THEN s.brand ELSE 'other' END AS brand_group\n",
    "FROM sampled_stations s\n",
    "LEFT JOIN brand_counts bc USING (brand);\n",
    "\"\"\")\n",
    "\n",
    "con.sql(\"SELECT brand_group, count(*) as n FROM stations_final_sample GROUP BY 1 ORDER BY 2 DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbdcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Price Gridding and Forward Fill\n",
    "con.execute(\"DROP TABLE IF EXISTS grid_sampled_e5_prepared;\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE grid_sampled_e5_prepared AS\n",
    "WITH \n",
    "-- 1. Filter prices to our 500 stations and E5 fuel only\n",
    "relevant_prices AS (\n",
    "    SELECT \n",
    "        p.station_uuid,\n",
    "        p.date as ts_raw,\n",
    "        CAST(p.e5 AS DOUBLE) as price_raw\n",
    "    FROM prices_raw p\n",
    "    WHERE p.station_uuid IN (SELECT station_uuid FROM stations_final_sample)\n",
    "      AND p.e5 IS NOT NULL AND p.e5 > 0\n",
    "),\n",
    "-- 2. Round timestamps to 30-minute floor, take average if multiple updates happen in 30 min\n",
    "prices_30min AS (\n",
    "    SELECT \n",
    "        station_uuid,\n",
    "        date_trunc('hour', ts_raw) + \n",
    "            INTERVAL (CASE WHEN EXTRACT(MINUTE FROM ts_raw) < 30 THEN 0 ELSE 30 END) MINUTE AS ts_30,\n",
    "        AVG(price_raw) as price_event\n",
    "    FROM relevant_prices\n",
    "    GROUP BY 1, 2\n",
    "),\n",
    "-- 3. Build the perfect Time Grid per station (min to max timestamp)\n",
    "station_bounds AS (\n",
    "    SELECT station_uuid, MIN(ts_30) as min_ts, MAX(ts_30) as max_ts \n",
    "    FROM prices_30min GROUP BY 1\n",
    "),\n",
    "full_grid AS (\n",
    "    SELECT sb.station_uuid, gs.ts_30\n",
    "    FROM station_bounds sb,\n",
    "    generate_series(sb.min_ts, sb.max_ts, INTERVAL 30 MINUTE) AS gs(ts_30)\n",
    "),\n",
    "-- 4. Join Grid with Events and Forward Fill (Optimized)\n",
    "joined_grid AS (\n",
    "    SELECT \n",
    "        g.station_uuid,\n",
    "        g.ts_30,\n",
    "        p.price_event\n",
    "    FROM full_grid g\n",
    "    LEFT JOIN prices_30min p ON g.station_uuid = p.station_uuid AND g.ts_30 = p.ts_30\n",
    ")\n",
    "-- Final Selection with Forward Fill\n",
    "SELECT\n",
    "    station_uuid,\n",
    "    ts_30,\n",
    "    -- MAGIC: Forward fill ignoring nulls replaces the complex recursive logic\n",
    "    LAST_VALUE(price_event IGNORE NULLS) OVER (\n",
    "        PARTITION BY station_uuid ORDER BY ts_30\n",
    "    ) AS price\n",
    "FROM joined_grid;\n",
    "\"\"\")\n",
    "\n",
    "# Validation: Ensure we didn't lose data\n",
    "con.sql(\"SELECT COUNT(*) FROM grid_sampled_e5_prepared WHERE price IS NOT NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8404c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Feature Engineering and Export\n",
    "con.execute(\"DROP TABLE IF EXISTS features_final;\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE features_final AS\n",
    "WITH prepared_with_time AS (\n",
    "    SELECT \n",
    "        g.*,\n",
    "        -- Convert to Berlin Time\n",
    "        timezone('Europe/Berlin', g.ts_30) AS ts_local,\n",
    "        CAST(timezone('Europe/Berlin', g.ts_30) AS DATE) AS d,\n",
    "        -- Calculate Time Cell (0-47)\n",
    "        (EXTRACT(HOUR FROM timezone('Europe/Berlin', g.ts_30)) * 2) + \n",
    "        (EXTRACT(MINUTE FROM timezone('Europe/Berlin', g.ts_30)) / 30) AS time_cell\n",
    "    FROM grid_sampled_e5_prepared g\n",
    "    WHERE g.price IS NOT NULL -- Remove leading nulls where forward fill hadn't started\n",
    ")\n",
    "SELECT \n",
    "    w.station_uuid,\n",
    "    w.d AS date,\n",
    "    w.time_cell,\n",
    "    w.price,\n",
    "    \n",
    "    -- 1. Seasonality\n",
    "    dayofweek(w.d) AS dow,\n",
    "    CASE WHEN dayofweek(w.d) IN (0, 6) THEN 1 ELSE 0 END AS is_weekend,\n",
    "    \n",
    "    -- 2. Lags (Raw History)\n",
    "    LAG(w.price, 1) OVER (PARTITION BY w.station_uuid, w.time_cell ORDER BY w.d) AS price_lag_1d,\n",
    "    LAG(w.price, 2) OVER (PARTITION BY w.station_uuid, w.time_cell ORDER BY w.d) AS price_lag_2d,\n",
    "    LAG(w.price, 3) OVER (PARTITION BY w.station_uuid, w.time_cell ORDER BY w.d) AS price_lag_3d,\n",
    "    LAG(w.price, 7) OVER (PARTITION BY w.station_uuid, w.time_cell ORDER BY w.d) AS price_lag_7d,\n",
    "    LAG(w.price, 14) OVER (PARTITION BY w.station_uuid, w.time_cell ORDER BY w.d) AS price_lag_14d,\n",
    "    LAG(w.price, 21) OVER (PARTITION BY w.station_uuid, w.time_cell ORDER BY w.d) AS price_lag_21d,\n",
    "    \n",
    "    -- 3. Brand Info\n",
    "    s.brand_group\n",
    "    \n",
    "FROM prepared_with_time w\n",
    "JOIN stations_final_sample s ON s.station_uuid = w.station_uuid\n",
    "-- Apply Filter after window functions\n",
    "QUALIFY price_lag_1d IS NOT NULL AND price_lag_21d IS NOT NULL;\n",
    "\"\"\")\n",
    "\n",
    "# --- ADD MOMENTUM (Calculated on the result to keep SQL clean) ---\n",
    "# Momentum: \"Are we spiking compared to history?\"\n",
    "con.execute(\"\"\"\n",
    "ALTER TABLE features_final ADD COLUMN mom_1d_2d DOUBLE;\n",
    "ALTER TABLE features_final ADD COLUMN mom_1d_7d DOUBLE;\n",
    "\n",
    "UPDATE features_final SET \n",
    "    mom_1d_2d = price_lag_1d - price_lag_2d,\n",
    "    mom_1d_7d = price_lag_1d - price_lag_7d;\n",
    "\"\"\")\n",
    "\n",
    "# Export\n",
    "output_dir = OUTPUT_PARQUET.parent\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Exporting to {OUTPUT_PARQUET}...\")\n",
    "con.execute(f\"COPY features_final TO '{OUTPUT_PARQUET}' (FORMAT PARQUET, COMPRESSION ZSTD);\")\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
