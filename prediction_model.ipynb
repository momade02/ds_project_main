{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00911616",
   "metadata": {},
   "source": [
    "# Data Science Project: Fuel Price Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2972974",
   "metadata": {},
   "source": [
    "### 3-Sentence summary of full project:\n",
    "\n",
    "We build a route-aware refueling recommender for Germany that forecasts station-level prices at the ETA and then optimizes “where/when to stop (and how much to buy)” by trading cheaper price vs. detour/time and fuel constraints. The pipeline ingests historical + live data, enriches with routing/ETA, weather, holidays, and macro signals, and serves uncertainty-aware recommendations via a FastAPI backend and an interactive Streamlit map. Everything is fully automated, validated, and documented in a reproducible repo, with the app hosted on the bwCloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb2828",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "from pathlib import Path\n",
    "\n",
    "# Base directory of the cloned tankerkoenig-data repo\n",
    "BASE_DIR = Path(\n",
    "    r\"C:\\Users\\websi\\OneDrive - UT Cloud\\Semester\\3. WS2025_26\\DS500 Data Science Project (12 ECTS)\\tankerkoenig_repo\\tankerkoenig-data\"\n",
    ")\n",
    "\n",
    "# DuckDB database file (will be created if it does not exist)\n",
    "DB_PATH = BASE_DIR / \"fuel_price_preparation.duckdb\"\n",
    "\n",
    "con = duckdb.connect(DB_PATH.as_posix())\n",
    "con.execute(\"PRAGMA threads=8;\")  # use multiple cores if available\n",
    "\n",
    "print(f\"Connected to DuckDB at: {DB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globs for 2023/2024 prices and stations\n",
    "prices_2023_glob = (BASE_DIR / \"prices\" / \"2023\" / \"*\" / \"*-prices.csv\").as_posix()\n",
    "prices_2024_glob = (BASE_DIR / \"prices\" / \"2024\" / \"*\" / \"*-prices.csv\").as_posix()\n",
    "\n",
    "stations_2023_glob = (BASE_DIR / \"stations\" / \"2023\" / \"*\" / \"*-stations.csv\").as_posix()\n",
    "stations_2024_glob = (BASE_DIR / \"stations\" / \"2024\" / \"*\" / \"*-stations.csv\").as_posix()\n",
    "\n",
    "print(prices_2023_glob)\n",
    "print(prices_2024_glob)\n",
    "print(stations_2023_glob)\n",
    "print(stations_2024_glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9644b32f0541749bd38bf5578de740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2.1 Prices: read 2023 + 2024 into one table\n",
    "con.execute(\"DROP TABLE IF EXISTS prices_raw;\")\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE prices_raw AS\n",
    "    SELECT * FROM read_csv_auto(?, SAMPLE_SIZE=-1)\n",
    "    UNION ALL\n",
    "    SELECT * FROM read_csv_auto(?, SAMPLE_SIZE=-1);\n",
    "    \"\"\",\n",
    "    [prices_2023_glob, prices_2024_glob],\n",
    ")\n",
    "\n",
    "# Quick sanity check\n",
    "con.execute(\"SELECT COUNT(*) AS n_rows, MIN(date), MAX(date) FROM prices_raw;\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fadfe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Stations: read 2023 + 2024 into one table, also keep filename\n",
    "con.execute(\"DROP TABLE IF EXISTS stations_raw;\")\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE stations_raw AS\n",
    "    SELECT\n",
    "        uuid,\n",
    "        name,\n",
    "        brand,\n",
    "        street,\n",
    "        house_number,\n",
    "        post_code,\n",
    "        city,\n",
    "        latitude,\n",
    "        longitude,\n",
    "        first_active,\n",
    "        openingtimes_json,\n",
    "        filename\n",
    "    FROM read_csv_auto(\n",
    "            ?, \n",
    "            SAMPLE_SIZE=-1, \n",
    "            filename=true, \n",
    "            union_by_name=true\n",
    "         )\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        uuid,\n",
    "        name,\n",
    "        brand,\n",
    "        street,\n",
    "        house_number,\n",
    "        post_code,\n",
    "        city,\n",
    "        latitude,\n",
    "        longitude,\n",
    "        first_active,\n",
    "        openingtimes_json,\n",
    "        filename\n",
    "    FROM read_csv_auto(\n",
    "            ?, \n",
    "            SAMPLE_SIZE=-1, \n",
    "            filename=true, \n",
    "            union_by_name=true\n",
    "         );\n",
    "    \"\"\",\n",
    "    [stations_2023_glob, stations_2024_glob],\n",
    ")\n",
    "\n",
    "con.execute(\n",
    "    \"SELECT COUNT(*) AS n_rows, MIN(uuid) AS min_uuid, MAX(uuid) AS max_uuid FROM stations_raw;\"\n",
    ").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Pick latest snapshot per station within 2023–2024\n",
    "con.execute(\"DROP TABLE IF EXISTS stations_snapshot;\")\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE stations_snapshot AS\n",
    "    WITH parsed AS (\n",
    "        SELECT\n",
    "            *,\n",
    "            -- extract 'YYYY-MM-DD' from filename, e.g. '.../2023-01-01-stations.csv'\n",
    "            CAST(regexp_extract(filename, '([0-9]{4}-[0-9]{2}-[0-9]{2})', 1) AS DATE) AS snapshot_date\n",
    "        FROM stations_raw\n",
    "    ),\n",
    "    ranked AS (\n",
    "        SELECT\n",
    "            *,\n",
    "            ROW_NUMBER() OVER (PARTITION BY uuid ORDER BY snapshot_date DESC) AS rn\n",
    "        FROM parsed\n",
    "    )\n",
    "    SELECT\n",
    "        uuid,\n",
    "        name,\n",
    "        brand,\n",
    "        street,\n",
    "        house_number,\n",
    "        post_code,\n",
    "        city,\n",
    "        CAST(latitude AS DOUBLE)  AS latitude,\n",
    "        CAST(longitude AS DOUBLE) AS longitude,\n",
    "        first_active,\n",
    "        openingtimes_json\n",
    "    FROM ranked\n",
    "    WHERE rn = 1;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "con.execute(\n",
    "    \"SELECT COUNT(*) AS n_rows, COUNT(DISTINCT uuid) AS n_uuids FROM stations_snapshot;\"\n",
    ").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Restrict to Berlin stations and compute brand groups (rare brands -> 'other')\n",
    "con.execute(\"DROP TABLE IF EXISTS stations_berlin;\")\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE stations_berlin AS\n",
    "    SELECT\n",
    "        uuid,\n",
    "        brand,\n",
    "        post_code,\n",
    "        city\n",
    "    FROM stations_snapshot\n",
    "    WHERE lower(city) = 'berlin';\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Brand grouping: brands with < 5 stations in Berlin are mapped to 'other'\n",
    "con.execute(\"DROP TABLE IF EXISTS stations_berlin_grouped;\")\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE stations_berlin_grouped AS\n",
    "    WITH brand_counts AS (\n",
    "        SELECT brand, COUNT(*) AS n\n",
    "        FROM stations_berlin\n",
    "        GROUP BY brand\n",
    "    ),\n",
    "    extended AS (\n",
    "        SELECT\n",
    "            s.uuid,\n",
    "            s.post_code,\n",
    "            s.city,\n",
    "            CASE WHEN bc.n >= 5 THEN s.brand ELSE 'other' END AS brand_group\n",
    "        FROM stations_berlin s\n",
    "        LEFT JOIN brand_counts bc USING (brand)\n",
    "    )\n",
    "    SELECT * FROM extended;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "con.execute(\n",
    "    \"SELECT brand_group, COUNT(*) AS n_stations FROM stations_berlin_grouped GROUP BY brand_group ORDER BY n_stations DESC;\"\n",
    ").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Berlin E5 price changes\n",
    "con.execute(\"DROP TABLE IF EXISTS prices_berlin_e5;\")\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE prices_berlin_e5 AS\n",
    "    SELECT\n",
    "        CAST(p.date AS TIMESTAMP) AS ts,\n",
    "        p.station_uuid,\n",
    "        CAST(p.e5 AS DOUBLE) AS price_e5\n",
    "    FROM prices_raw p\n",
    "    JOIN stations_berlin_grouped s\n",
    "      ON s.uuid = p.station_uuid\n",
    "    WHERE p.e5 IS NOT NULL AND p.e5 > 0;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "con.execute(\n",
    "    \"SELECT COUNT(*) AS n_rows, MIN(ts) AS min_ts, MAX(ts) AS max_ts FROM prices_berlin_e5;\"\n",
    ").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Round timestamps down to the nearest 30-minute grid cell\n",
    "con.execute(\"DROP TABLE IF EXISTS prices_berlin_e5_rounded;\")\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE prices_berlin_e5_rounded AS\n",
    "    SELECT\n",
    "        station_uuid,\n",
    "        -- floor to 30-minute grid: 00 or 30\n",
    "        date_trunc('hour', ts)\n",
    "          + INTERVAL (CASE WHEN EXTRACT(MINUTE FROM ts) < 30 THEN 0 ELSE 30 END) MINUTE AS ts_30,\n",
    "        AVG(price_e5) AS price_e5\n",
    "    FROM prices_berlin_e5\n",
    "    GROUP BY station_uuid, ts_30;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "con.execute(\n",
    "    \"SELECT COUNT(*) AS n_rows, MIN(ts_30) AS min_ts, MAX(ts_30) AS max_ts FROM prices_berlin_e5_rounded;\"\n",
    ").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"DROP TABLE IF EXISTS grid_berlin_e5;\")\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE grid_berlin_e5 AS\n",
    "    WITH station_span AS (\n",
    "        SELECT\n",
    "            station_uuid,\n",
    "            MIN(ts_30) AS min_ts,\n",
    "            MAX(ts_30) AS max_ts\n",
    "        FROM prices_berlin_e5_rounded\n",
    "        GROUP BY station_uuid\n",
    "    ),\n",
    "    grid AS (\n",
    "        -- 30-minute grid per station from first to last observed change\n",
    "        SELECT\n",
    "            s.station_uuid,\n",
    "            gs.ts_30\n",
    "        FROM station_span s,\n",
    "             generate_series(\n",
    "                 s.min_ts,\n",
    "                 s.max_ts,\n",
    "                 INTERVAL 30 MINUTE\n",
    "             ) AS gs(ts_30)\n",
    "    ),\n",
    "    base AS (\n",
    "        -- attach price events (may be NULL if no change in this grid cell)\n",
    "        SELECT\n",
    "            g.station_uuid,\n",
    "            g.ts_30,\n",
    "            pr.price_e5 AS price_event\n",
    "        FROM grid g\n",
    "        LEFT JOIN prices_berlin_e5_rounded pr\n",
    "          ON pr.station_uuid = g.station_uuid\n",
    "         AND pr.ts_30 = g.ts_30\n",
    "    ),\n",
    "    numbered AS (\n",
    "        -- sequential index per station for forward-fill logic\n",
    "        SELECT\n",
    "            station_uuid,\n",
    "            ts_30,\n",
    "            price_event,\n",
    "            ROW_NUMBER() OVER (PARTITION BY station_uuid ORDER BY ts_30) AS k\n",
    "        FROM base\n",
    "    ),\n",
    "    with_last_k AS (\n",
    "        -- for each row, compute index of last row with a non-null price_event\n",
    "        SELECT\n",
    "            station_uuid,\n",
    "            ts_30,\n",
    "            k,\n",
    "            MAX(CASE WHEN price_event IS NOT NULL THEN k ELSE NULL END) OVER (\n",
    "                PARTITION BY station_uuid\n",
    "                ORDER BY k\n",
    "                ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "            ) AS last_k\n",
    "        FROM numbered\n",
    "    ),\n",
    "    ff AS (\n",
    "        -- join back to get the forward-filled price\n",
    "        SELECT\n",
    "            n.station_uuid,\n",
    "            n.ts_30,\n",
    "            e.price_event AS price_e5\n",
    "        FROM with_last_k n\n",
    "        LEFT JOIN numbered e\n",
    "          ON e.station_uuid = n.station_uuid\n",
    "         AND e.k = n.last_k\n",
    "    )\n",
    "    SELECT *\n",
    "    FROM ff\n",
    "    WHERE price_e5 IS NOT NULL;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "con.execute(\n",
    "    \"SELECT COUNT(*) AS n_rows, MIN(ts_30) AS min_ts, MAX(ts_30) AS max_ts FROM grid_berlin_e5;\"\n",
    ").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"DROP TABLE IF EXISTS grid_berlin_e5_prepared;\")\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE grid_berlin_e5_prepared AS\n",
    "    SELECT\n",
    "        station_uuid,\n",
    "        ts_30 AS ts,\n",
    "        CAST(ts_30 AS DATE) AS d,                                  -- calendar date\n",
    "        (EXTRACT(HOUR FROM ts_30) * 2 + EXTRACT(MINUTE FROM ts_30) / 30) AS time_cell,  -- 0..47\n",
    "        price_e5 AS price\n",
    "    FROM grid_berlin_e5;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        MIN(d) AS min_date,\n",
    "        MAX(d) AS max_date,\n",
    "        MIN(time_cell) AS min_cell,\n",
    "        MAX(time_cell) AS max_cell\n",
    "    FROM grid_berlin_e5_prepared;\n",
    "    \"\"\"\n",
    ").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"DROP TABLE IF EXISTS features_raw;\")\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE features_raw AS\n",
    "    SELECT\n",
    "        g.station_uuid,\n",
    "        g.d,\n",
    "        g.time_cell,\n",
    "        g.price AS price_today,\n",
    "        y.price AS price_yesterday,\n",
    "        w.price AS price_weekago\n",
    "    FROM grid_berlin_e5_prepared g\n",
    "    LEFT JOIN grid_berlin_e5_prepared y\n",
    "      ON y.station_uuid = g.station_uuid\n",
    "     AND y.time_cell   = g.time_cell\n",
    "     AND y.d           = g.d - INTERVAL 1 DAY\n",
    "    LEFT JOIN grid_berlin_e5_prepared w\n",
    "      ON w.station_uuid = g.station_uuid\n",
    "     AND w.time_cell   = g.time_cell\n",
    "     AND w.d           = g.d - INTERVAL 7 DAY;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) AS n_rows,\n",
    "        SUM(CASE WHEN price_yesterday IS NOT NULL AND price_weekago IS NOT NULL THEN 1 ELSE 0 END) AS n_complete\n",
    "    FROM features_raw;\n",
    "    \"\"\"\n",
    ").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf27a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"DROP TABLE IF EXISTS features_berlin_e5;\")\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE features_berlin_e5 AS\n",
    "    SELECT\n",
    "        f.station_uuid,\n",
    "        f.d      AS date,\n",
    "        f.time_cell,\n",
    "        f.price_today    AS price,\n",
    "        f.price_yesterday,\n",
    "        f.price_weekago,\n",
    "        sb.brand_group,\n",
    "        sb.post_code,\n",
    "        EXTRACT(DOW FROM f.d) AS day_of_week  -- 0=Sunday ... 6=Saturday\n",
    "    FROM features_raw f\n",
    "    JOIN stations_berlin_grouped sb\n",
    "      ON sb.uuid = f.station_uuid\n",
    "    WHERE f.price_yesterday IS NOT NULL\n",
    "      AND f.price_weekago  IS NOT NULL;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) AS n_rows,\n",
    "        MIN(date) AS min_date,\n",
    "        MAX(date) AS max_date\n",
    "    FROM features_berlin_e5;\n",
    "    \"\"\"\n",
    ").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f0d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = con.execute(\n",
    "    \"SELECT * FROM features_berlin_e5 ORDER BY date, time_cell, station_uuid LIMIT 20;\"\n",
    ").df()\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c46a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.x Load EU Weekly Oil Bulletin (market-level prices) into DuckDB ---\n",
    "\n",
    "market_csv = BASE_DIR / \"market_level_fuel_price.csv\"  # adjust name if needed\n",
    "\n",
    "con.execute(\"DROP TABLE IF EXISTS market_weekly;\")\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE market_weekly AS\n",
    "    SELECT\n",
    "        -- 'date' is already a DATE from read_csv_auto\n",
    "        CAST(date AS DATE)          AS date,\n",
    "        CAST(euro95_1000l AS DOUBLE) AS euro95_1000l,\n",
    "        CAST(diesel_1000l AS DOUBLE) AS diesel_1000l\n",
    "    FROM read_csv_auto(?, SAMPLE_SIZE=-1);\n",
    "    \"\"\",\n",
    "    [market_csv.as_posix()],\n",
    ")\n",
    "\n",
    "# Quick check\n",
    "con.execute(\n",
    "    \"SELECT MIN(date) AS min_date, MAX(date) AS max_date, COUNT(*) AS n_rows FROM market_weekly;\"\n",
    ").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934afd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.x Create daily Euro95 market index + lagged versions ---\n",
    "\n",
    "# 1) Daily series with forward-filled weekly values\n",
    "con.execute(\"DROP TABLE IF EXISTS market_daily;\")\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE market_daily AS\n",
    "    WITH days AS (\n",
    "        -- generate one row per calendar day between first and last bulletin\n",
    "        SELECT\n",
    "            gs AS date\n",
    "        FROM generate_series(\n",
    "            (SELECT MIN(date) FROM market_weekly),\n",
    "            (SELECT MAX(date) FROM market_weekly),\n",
    "            INTERVAL 1 DAY\n",
    "        ) AS t(gs)\n",
    "    ),\n",
    "    joined AS (\n",
    "        -- join weekly quotes onto the daily calendar\n",
    "        SELECT\n",
    "            d.date,\n",
    "            w.euro95_1000l\n",
    "        FROM days d\n",
    "        LEFT JOIN market_weekly w\n",
    "          ON w.date = d.date\n",
    "    ),\n",
    "    grp AS (\n",
    "        -- running count of non-null quotes; constant within each bulletin interval\n",
    "        SELECT\n",
    "            date,\n",
    "            euro95_1000l,\n",
    "            COUNT(CASE WHEN euro95_1000l IS NOT NULL THEN 1 END)\n",
    "                OVER (ORDER BY date) AS grp\n",
    "        FROM joined\n",
    "    ),\n",
    "    ff AS (\n",
    "        -- forward-filled value: within each grp use the (only) non-null quote\n",
    "        SELECT\n",
    "            date,\n",
    "            MAX(euro95_1000l) OVER (PARTITION BY grp) AS euro95_1000l\n",
    "        FROM grp\n",
    "    )\n",
    "    SELECT *\n",
    "    FROM ff\n",
    "    ORDER BY date;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# sanity check\n",
    "con.execute(\n",
    "    \"SELECT MIN(date) AS min_date, MAX(date) AS max_date, COUNT(*) AS n_rows FROM market_daily;\"\n",
    ").df()\n",
    "\n",
    "# 2) Add daily lags: market_yesterday (1 day), market_weekago (7 days)\n",
    "con.execute(\"DROP TABLE IF EXISTS market_daily_lags;\")\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE market_daily_lags AS\n",
    "    SELECT\n",
    "        date,\n",
    "        euro95_1000l,\n",
    "        lag(euro95_1000l, 1) OVER (ORDER BY date) AS market_yesterday,\n",
    "        lag(euro95_1000l, 7) OVER (ORDER BY date) AS market_weekago\n",
    "    FROM market_daily\n",
    "    ORDER BY date;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "con.execute(\n",
    "    \"SELECT * FROM market_daily_lags ORDER BY date LIMIT 10;\"\n",
    ").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b6b898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.x Enrich features_berlin_e5 with market-level variables and markups ---\n",
    "\n",
    "con.execute(\"DROP TABLE IF EXISTS features_berlin_e5_market;\")\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE features_berlin_e5_market AS\n",
    "    SELECT\n",
    "        f.station_uuid,\n",
    "        f.date,\n",
    "        f.time_cell,\n",
    "        f.price,\n",
    "        f.price_yesterday,\n",
    "        f.price_weekago,\n",
    "        f.brand_group,\n",
    "        f.post_code,\n",
    "        f.day_of_week,\n",
    "        m.market_yesterday,\n",
    "        m.market_weekago,\n",
    "        -- ECM-style markup terms (station price minus market level)\n",
    "        f.price_yesterday - m.market_yesterday AS markup_yesterday,\n",
    "        f.price_weekago  - m.market_weekago  AS markup_weekago\n",
    "    FROM features_berlin_e5 f\n",
    "    LEFT JOIN market_daily_lags m\n",
    "      ON f.date = m.date\n",
    "    WHERE m.market_yesterday IS NOT NULL\n",
    "      AND m.market_weekago IS NOT NULL\n",
    "    ;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Sanity check\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    SELECT COUNT(*) AS n_rows,\n",
    "           MIN(date) AS min_date,\n",
    "           MAX(date) AS max_date\n",
    "    FROM features_berlin_e5_market;\n",
    "    \"\"\"\n",
    ").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5328d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.x Export enriched features (with market) to Parquet ---\n",
    "\n",
    "output_dir = BASE_DIR / \"derived\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "output_parquet = output_dir / \"features_berlin_e5_market_2023_2024.parquet\"\n",
    "\n",
    "sql = f\"\"\"\n",
    "COPY features_berlin_e5_market\n",
    "TO '{output_parquet.as_posix()}'\n",
    "(FORMAT PARQUET, COMPRESSION ZSTD);\n",
    "\"\"\"\n",
    "con.execute(sql)\n",
    "\n",
    "print(f\"Exported features with market to: {output_parquet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8d9a3a",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ae57c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Path to enriched features parquet\n",
    "BASE_DIR = Path(\n",
    "    r\"C:\\Users\\websi\\OneDrive - UT Cloud\\Semester\\3. WS2025_26\\DS500 Data Science Project (12 ECTS)\\tankerkoenig_repo\\tankerkoenig-data\"\n",
    ")\n",
    "features_path = BASE_DIR / \"derived\" / \"features_berlin_e5_market_2023_2024.parquet\"\n",
    "\n",
    "df = pd.read_parquet(features_path)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6680f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast dtypes for clarity / efficiency\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df[\"time_cell\"] = df[\"time_cell\"].astype(\"int16\")\n",
    "df[\"day_of_week\"] = df[\"day_of_week\"].astype(\"int8\")\n",
    "df[\"post_code\"] = df[\"post_code\"].astype(str)\n",
    "\n",
    "# Optional: sort by time (not strictly needed but nice to have)\n",
    "df = df.sort_values([\"date\", \"time_cell\", \"station_uuid\"]).reset_index(drop=True)\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28481aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW: intraday last-price feature (per station & day) ---\n",
    "\n",
    "# make sure rows are ordered correctly\n",
    "df = df.sort_values([\"station_uuid\", \"date\", \"time_cell\"]).reset_index(drop=True)\n",
    "\n",
    "# price in the *previous* time cell on the same day for the same station\n",
    "df[\"price_intraday_prev\"] = (\n",
    "    df\n",
    "    .groupby([\"station_uuid\", \"date\"])[\"price\"]\n",
    "    .shift(1)\n",
    ")\n",
    "\n",
    "# simple and safe missing-value handling:\n",
    "# for the first time_cell of each day there is no \"previous cell\"\n",
    "# -> fall back to yesterday's price at this time_cell\n",
    "df[\"price_intraday_prev\"] = df[\"price_intraday_prev\"].fillna(df[\"price_yesterday\"])\n",
    "\n",
    "# optional: ensure numeric type consistent with other price features\n",
    "df[\"price_intraday_prev\"] = df[\"price_intraday_prev\"].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cut dates\n",
    "train_end = pd.Timestamp(\"2024-06-30\")\n",
    "valid_end = pd.Timestamp(\"2024-09-30\")\n",
    "\n",
    "train_mask = df[\"date\"] <= train_end\n",
    "valid_mask = (df[\"date\"] > train_end) & (df[\"date\"] <= valid_end)\n",
    "test_mask  = df[\"date\"] > valid_end\n",
    "\n",
    "print(\"Train rows:\", train_mask.sum())\n",
    "print(\"Valid rows:\", valid_mask.sum())\n",
    "print(\"Test rows :\", test_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3ffdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define target and features ---\n",
    "\n",
    "target_col = \"price\"\n",
    "\n",
    "feature_cols = [\n",
    "    \"time_cell\",\n",
    "    \"day_of_week\",\n",
    "    \"post_code\",\n",
    "    \"brand_group\",\n",
    "    \"price_intraday_prev\",   # NEW: intraday last price (same day, same station)\n",
    "    \"price_yesterday\",\n",
    "    \"price_weekago\",\n",
    "    \"market_yesterday\",\n",
    "    \"market_weekago\",\n",
    "    \"markup_yesterday\",\n",
    "    \"markup_weekago\",\n",
    "]\n",
    "\n",
    "cat_cols = [\"brand_group\", \"post_code\"]  # categorical for LightGBM\n",
    "\n",
    "# --- Time-based train/validation/test split (as before) ---\n",
    "\n",
    "train_end = pd.Timestamp(\"2024-06-30\")\n",
    "valid_end = pd.Timestamp(\"2024-09-30\")\n",
    "\n",
    "train_mask = df[\"date\"] <= train_end\n",
    "valid_mask = (df[\"date\"] > train_end) & (df[\"date\"] <= valid_end)\n",
    "test_mask  = df[\"date\"] > valid_end\n",
    "\n",
    "print(\"Train rows:\", train_mask.sum())\n",
    "print(\"Valid rows:\", valid_mask.sum())\n",
    "print(\"Test rows :\", test_mask.sum())\n",
    "\n",
    "X_train = df.loc[train_mask, feature_cols].copy()\n",
    "y_train = df.loc[train_mask, target_col].values\n",
    "\n",
    "X_valid = df.loc[valid_mask, feature_cols].copy()\n",
    "y_valid = df.loc[valid_mask, target_col].values\n",
    "\n",
    "X_test  = df.loc[test_mask,  feature_cols].copy()\n",
    "y_test  = df.loc[test_mask,  target_col].values\n",
    "\n",
    "# Cast categoricals\n",
    "for c in cat_cols:\n",
    "    X_train[c] = X_train[c].astype(\"category\")\n",
    "    X_valid[c] = X_valid[c].astype(\"category\")\n",
    "    X_test[c]  = X_test[c].astype(\"category\")\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629414a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def mae_rmse(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)  # always returns MSE\n",
    "    rmse = np.sqrt(mse)\n",
    "    return mae, rmse\n",
    "\n",
    "# Baseline 1: yesterday same time\n",
    "b1_pred = df.loc[test_mask, \"price_yesterday\"].values\n",
    "b1_mae, b1_rmse = mae_rmse(y_test, b1_pred)\n",
    "\n",
    "# Baseline 2: week-ago same time\n",
    "b2_pred = df.loc[test_mask, \"price_weekago\"].values\n",
    "b2_mae, b2_rmse = mae_rmse(y_test, b2_pred)\n",
    "\n",
    "# Baseline 3: simple average of yesterday and week-ago\n",
    "b3_pred = 0.5 * (df.loc[test_mask, \"price_yesterday\"].values +\n",
    "                 df.loc[test_mask, \"price_weekago\"].values)\n",
    "b3_mae, b3_rmse = mae_rmse(y_test, b3_pred)\n",
    "\n",
    "print(\"Baseline metrics on TEST set\")\n",
    "print(f\"Yesterday:   MAE={b1_mae:.4f}, RMSE={b1_rmse:.4f}\")\n",
    "print(f\"Week-ago:    MAE={b2_mae:.4f}, RMSE={b2_rmse:.4f}\")\n",
    "print(f\"Avg(y,7d):   MAE={b3_mae:.4f}, RMSE={b3_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab92d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(\n",
    "    X_train,\n",
    "    label=y_train,\n",
    "    categorical_feature=cat_cols,\n",
    "    free_raw_data=False,\n",
    ")\n",
    "\n",
    "valid_data = lgb.Dataset(\n",
    "    X_valid,\n",
    "    label=y_valid,\n",
    "    categorical_feature=cat_cols,\n",
    "    reference=train_data,\n",
    "    free_raw_data=False,\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"mae\",      # L1 regression\n",
    "    \"metric\": \"mae\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 64,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"min_data_in_leaf\": 200,\n",
    "    \"max_bin\": 255,\n",
    "    \"verbosity\": -1,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "callbacks = [\n",
    "    lgb.early_stopping(stopping_rounds=100, verbose=True),\n",
    "    lgb.log_evaluation(period=50),  # optional: log every 50 iters\n",
    "]\n",
    "\n",
    "gbm = lgb.train(\n",
    "    params,\n",
    "    train_set=train_data,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=[\"train\", \"valid\"],\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd245e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best iteration determined by early stopping\n",
    "y_pred_test = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "\n",
    "lgb_mae, lgb_rmse = mae_rmse(y_test, y_pred_test)\n",
    "\n",
    "print(\"LightGBM metrics on TEST set\")\n",
    "print(f\"LightGBM:    MAE={lgb_mae:.4f}, RMSE={lgb_rmse:.4f}\")\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"Yesterday:   MAE={b1_mae:.4f}, RMSE={b1_rmse:.4f}\")\n",
    "print(f\"Week-ago:    MAE={b2_mae:.4f}, RMSE={b2_rmse:.4f}\")\n",
    "print(f\"Avg(y,7d):   MAE={b3_mae:.4f}, RMSE={b3_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce857690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importances = gbm.feature_importance(importance_type=\"gain\")\n",
    "for col, imp in sorted(zip(feature_cols, importances), key=lambda x: -x[1]):\n",
    "    print(f\"{col:15s} {imp:.1f}\")\n",
    "\n",
    "# Simple bar plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "order = np.argsort(importances)\n",
    "plt.barh(np.array(feature_cols)[order], importances[order])\n",
    "plt.xlabel(\"Importance (gain)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552c629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to disk for later use\n",
    "model_path = BASE_DIR / \"derived\" / \"lightgbm_berlin_e5_mae.txt\"\n",
    "gbm.save_model(model_path.as_posix())\n",
    "print(f\"Saved LightGBM model to: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
