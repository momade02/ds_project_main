{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ecba68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Connected to fuel_price_preparation.duckdb\n",
      "Connected to: C:\\Users\\websi\\OneDrive - UT Cloud\\Semester\\3. WS2025_26\\DS500 Data Science Project (12 ECTS)\\tankerkoenig_repo\\tankerkoenig-data\\fuel_price_preparation.duckdb\n"
     ]
    }
   ],
   "source": [
    "# 1. Configuration & Connection\n",
    "\n",
    "import duckdb\n",
    "import gc # Garbage Collector\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "BASE_DIR = Path(r\"C:\\Users\\websi\\OneDrive - UT Cloud\\Semester\\3. WS2025_26\\DS500 Data Science Project (12 ECTS)\\tankerkoenig_repo\\tankerkoenig-data\")\n",
    "DB_PATH = BASE_DIR / \"fuel_price_preparation.duckdb\"\n",
    "OUTPUT_PARQUET = BASE_DIR / \"derived\" / \"features_sampled_diesel_2023_2024.parquet\"\n",
    "\n",
    "# Globs\n",
    "PRICE_GLOBS = [\n",
    "    str(BASE_DIR / \"prices\" / \"2023\" / \"*\" / \"*-prices.csv\"),\n",
    "    str(BASE_DIR / \"prices\" / \"2024\" / \"*\" / \"*-prices.csv\")\n",
    "]\n",
    "STATION_GLOBS = [\n",
    "    str(BASE_DIR / \"stations\" / \"2023\" / \"*\" / \"*-stations.csv\"),\n",
    "    str(BASE_DIR / \"stations\" / \"2024\" / \"*\" / \"*-stations.csv\")\n",
    "]\n",
    "\n",
    "# --- ROBUST CONNECTION ---\n",
    "# 1. Force close any existing connection variables\n",
    "try:\n",
    "    if 'con' in globals():\n",
    "        con.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 2. Force Python to clear the memory (kill the \"Zombie\")\n",
    "gc.collect()\n",
    "\n",
    "# 3. Create the new connection\n",
    "try:\n",
    "    con = duckdb.connect(str(DB_PATH))\n",
    "    con.execute(\"PRAGMA threads=8;\")\n",
    "    con.execute(\"SELECT setseed(0.42);\")\n",
    "    print(f\"SUCCESS: Connected to {DB_PATH.name}\")\n",
    "except Exception as e:\n",
    "    print(\"ERROR: Could not connect. If using OneDrive, pause syncing or restart the kernel.\")\n",
    "    print(e)\n",
    "\n",
    "print(f\"Connected to: {DB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193639e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee05a4980574615bfda0dab404c6211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37252af2d8f848f5b3fcc26dabbd1ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion complete.\n",
      "┌──────────┬───────────┐\n",
      "│   type   │     n     │\n",
      "│ varchar  │   int64   │\n",
      "├──────────┼───────────┤\n",
      "│ Prices   │ 273916339 │\n",
      "│ Stations │  12523064 │\n",
      "└──────────┴───────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Data Ingestion (Bronze Layer)\n",
    "\n",
    "# --- Prices ---\n",
    "con.execute(\"DROP TABLE IF EXISTS prices_raw;\")\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE prices_raw AS\n",
    "    SELECT *\n",
    "    FROM read_csv_auto(?, union_by_name = true);\n",
    "    \"\"\",\n",
    "    [PRICE_GLOBS],\n",
    ")\n",
    "\n",
    "# --- Stations ---\n",
    "con.execute(\"DROP TABLE IF EXISTS stations_raw;\")\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE stations_raw AS\n",
    "    SELECT *\n",
    "    FROM read_csv_auto(?, filename = true, union_by_name = true);\n",
    "    \"\"\",\n",
    "    [STATION_GLOBS],\n",
    ")\n",
    "\n",
    "print(\"Ingestion complete.\")\n",
    "con.sql(\n",
    "    \"\"\"\n",
    "    SELECT 'Prices' AS type, COUNT(*) AS n FROM prices_raw\n",
    "    UNION ALL\n",
    "    SELECT 'Stations', COUNT(*) FROM stations_raw;\n",
    "    \"\"\"\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bffbfb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827b10250fb84fc5962e76f35d1ab586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────┐\n",
      "│ n_stations │\n",
      "│   int64    │\n",
      "├────────────┤\n",
      "│        500 │\n",
      "└────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Station Processing: latest snapshot -> sample of 500 stations\n",
    "\n",
    "con.execute(\"DROP TABLE IF EXISTS stations_final_sample;\")\n",
    "\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE stations_final_sample AS\n",
    "    WITH parsed_stations AS (\n",
    "        SELECT\n",
    "            *,\n",
    "            CAST(\n",
    "                regexp_extract(filename, '([0-9]{4}-[0-9]{2}-[0-9]{2})', 1)\n",
    "                AS DATE\n",
    "            ) AS snapshot_date\n",
    "        FROM stations_raw\n",
    "    ),\n",
    "    latest_snapshot AS (\n",
    "        SELECT * EXCLUDE (rn)\n",
    "        FROM (\n",
    "            SELECT\n",
    "                *,\n",
    "                ROW_NUMBER() OVER (\n",
    "                    PARTITION BY uuid\n",
    "                    ORDER BY snapshot_date DESC\n",
    "                ) AS rn\n",
    "            FROM parsed_stations\n",
    "        )\n",
    "        WHERE rn = 1\n",
    "    ),\n",
    "    sampled_stations AS (\n",
    "        -- Random 500-station sample to keep data manageable\n",
    "        SELECT *\n",
    "        FROM latest_snapshot\n",
    "        ORDER BY random()\n",
    "        LIMIT 500\n",
    "    )\n",
    "    SELECT\n",
    "        s.uuid AS station_uuid\n",
    "    FROM sampled_stations s;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "con.sql(\"SELECT COUNT(*) AS n_stations FROM stations_final_sample\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "218f3e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131d3a902431439ea5e98eee36c009af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────┐\n",
      "│ n_non_null │\n",
      "│   int64    │\n",
      "├────────────┤\n",
      "│   15501865 │\n",
      "└────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Price Gridding and Forward Fill  (DIESEL only)\n",
    "\n",
    "con.execute(\"DROP TABLE IF EXISTS grid_sampled_diesel_prepared;\")\n",
    "\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE grid_sampled_diesel_prepared AS\n",
    "    WITH\n",
    "\n",
    "    -- 1. Filter prices to our 500 stations and diesel fuel only\n",
    "    relevant_prices AS (\n",
    "        SELECT\n",
    "            p.station_uuid,\n",
    "            CAST(p.date AS TIMESTAMP) AS ts_raw,            -- ensure scalar TIMESTAMP\n",
    "            CAST(p.diesel AS DOUBLE) AS price_raw\n",
    "        FROM prices_raw p\n",
    "        WHERE p.station_uuid IN (SELECT station_uuid FROM stations_final_sample)\n",
    "          AND p.diesel IS NOT NULL\n",
    "          AND p.diesel > 0\n",
    "    ),\n",
    "\n",
    "    -- 2. Round timestamps to 30-minute floor, take average if multiple updates in 30 min\n",
    "    prices_30min AS (\n",
    "        SELECT\n",
    "            station_uuid,\n",
    "            date_trunc('hour', ts_raw)\n",
    "                + INTERVAL (CASE WHEN EXTRACT(MINUTE FROM ts_raw) < 30\n",
    "                                  THEN 0\n",
    "                                  ELSE 30\n",
    "                             END) MINUTE AS ts_30,\n",
    "            AVG(price_raw) AS price_event\n",
    "        FROM relevant_prices\n",
    "        GROUP BY 1, 2\n",
    "    ),\n",
    "\n",
    "    -- 3. Build the per-station time grid (min to max timestamp)\n",
    "    station_bounds AS (\n",
    "        SELECT\n",
    "            station_uuid,\n",
    "            MIN(ts_30) AS min_ts,\n",
    "            MAX(ts_30) AS max_ts\n",
    "        FROM prices_30min\n",
    "        GROUP BY 1\n",
    "    ),\n",
    "\n",
    "    full_grid AS (\n",
    "        SELECT\n",
    "            sb.station_uuid,\n",
    "            gs.ts_30\n",
    "        FROM station_bounds sb,\n",
    "             -- IMPORTANT: table version of generate_series → scalar TIMESTAMP, not TIMESTAMP[]\n",
    "             generate_series(sb.min_ts, sb.max_ts, INTERVAL 30 MINUTE) AS gs(ts_30)\n",
    "    ),\n",
    "\n",
    "    -- 4. Join grid with events and forward fill\n",
    "    joined_grid AS (\n",
    "        SELECT\n",
    "            g.station_uuid,\n",
    "            g.ts_30,\n",
    "            p.price_event\n",
    "        FROM full_grid g\n",
    "        LEFT JOIN prices_30min p\n",
    "          ON g.station_uuid = p.station_uuid\n",
    "         AND g.ts_30 = p.ts_30\n",
    "    )\n",
    "\n",
    "    -- Final selection with forward fill\n",
    "    SELECT\n",
    "        station_uuid,\n",
    "        ts_30,\n",
    "        LAST_VALUE(price_event IGNORE NULLS) OVER (\n",
    "            PARTITION BY station_uuid\n",
    "            ORDER BY ts_30\n",
    "        ) AS price\n",
    "    FROM joined_grid;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Validation: ensure we didn't lose data\n",
    "con.sql(\"SELECT COUNT(*) AS n_non_null FROM grid_sampled_diesel_prepared WHERE price IS NOT NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dfe8cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc815755b1d42749ebc3fa2e8da998b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting to C:\\Users\\websi\\OneDrive - UT Cloud\\Semester\\3. WS2025_26\\DS500 Data Science Project (12 ECTS)\\tankerkoenig_repo\\tankerkoenig-data\\derived\\features_sampled_diesel_2023_2024.parquet ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9a8dda097548feacb36f4dbc20b447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# 5. Feature engineering: daily lags + intra-day (cell) lags, export to parquet\n",
    "\n",
    "con.execute(\"DROP TABLE IF EXISTS features_final;\")\n",
    "\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE features_final AS\n",
    "    WITH prepared_with_time AS (\n",
    "        SELECT\n",
    "            g.station_uuid,\n",
    "            g.ts_30,\n",
    "            timezone('Europe/Berlin', g.ts_30) AS ts_local,\n",
    "            CAST(timezone('Europe/Berlin', g.ts_30) AS DATE) AS date,\n",
    "            -- Time cell 0–47 (30-minute buckets)\n",
    "            (EXTRACT(HOUR   FROM timezone('Europe/Berlin', g.ts_30)) * 2) +\n",
    "            (EXTRACT(MINUTE FROM timezone('Europe/Berlin', g.ts_30)) / 30) AS time_cell,\n",
    "            g.price\n",
    "        FROM grid_sampled_diesel_prepared g\n",
    "        WHERE g.price IS NOT NULL\n",
    "    ),\n",
    "\n",
    "    -- DAILY LAGS: same time_cell on previous days\n",
    "    daily_lags AS (\n",
    "        SELECT\n",
    "            *,\n",
    "            LAG(price, 1) OVER (\n",
    "                PARTITION BY station_uuid, time_cell\n",
    "                ORDER BY date\n",
    "            ) AS price_lag_1d,\n",
    "            LAG(price, 2) OVER (\n",
    "                PARTITION BY station_uuid, time_cell\n",
    "                ORDER BY date\n",
    "            ) AS price_lag_2d,\n",
    "            LAG(price, 3) OVER (\n",
    "                PARTITION BY station_uuid, time_cell\n",
    "                ORDER BY date\n",
    "            ) AS price_lag_3d,\n",
    "            LAG(price, 7) OVER (\n",
    "                PARTITION BY station_uuid, time_cell\n",
    "                ORDER BY date\n",
    "            ) AS price_lag_7d\n",
    "        FROM prepared_with_time\n",
    "    ),\n",
    "\n",
    "    -- INTRA-DAY LAGS: previous cells on the SAME day (today's within-day history)\n",
    "    intraday_lags AS (\n",
    "        SELECT\n",
    "            *,\n",
    "            LAG(price, 1) OVER (\n",
    "                PARTITION BY station_uuid, date\n",
    "                ORDER BY time_cell\n",
    "            ) AS price_lag_1cell,\n",
    "            LAG(price, 2) OVER (\n",
    "                PARTITION BY station_uuid, date\n",
    "                ORDER BY time_cell\n",
    "            ) AS price_lag_2cell,\n",
    "            LAG(price, 3) OVER (\n",
    "                PARTITION BY station_uuid, date\n",
    "                ORDER BY time_cell\n",
    "            ) AS price_lag_3cell,\n",
    "            LAG(price, 4) OVER (\n",
    "                PARTITION BY station_uuid, date\n",
    "                ORDER BY time_cell\n",
    "            ) AS price_lag_4cell\n",
    "        FROM daily_lags\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "        station_uuid,\n",
    "        ts_30,\n",
    "        ts_local,\n",
    "        date,\n",
    "        time_cell,\n",
    "        price,\n",
    "        price_lag_1d,\n",
    "        price_lag_2d,\n",
    "        price_lag_3d,\n",
    "        price_lag_7d,\n",
    "        price_lag_1cell,\n",
    "        price_lag_2cell,\n",
    "        price_lag_3cell,\n",
    "        price_lag_4cell\n",
    "    FROM intraday_lags\n",
    "    WHERE price_lag_1d IS NOT NULL\n",
    "      AND price_lag_7d IS NOT NULL;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "output_dir = OUTPUT_PARQUET.parent\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Exporting to {OUTPUT_PARQUET} ...\")\n",
    "con.execute(\n",
    "    f\"COPY features_final TO '{OUTPUT_PARQUET}' (FORMAT PARQUET, COMPRESSION ZSTD);\"\n",
    ")\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
